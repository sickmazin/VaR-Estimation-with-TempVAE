Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_132615
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_132615

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 47
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.01
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_132615
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.1
free_bits: 2.0
gradient_clip: 2.5
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...
Epoch 0050 | Beta: 0.10 | Loss: 78.1890 | NLL: 46.1789 | KL: 336.3281
Epoch 0100 | Beta: 0.18 | Loss: 87.8277 | NLL: 42.6498 | KL: 246.9257
Epoch 0150 | Beta: 0.26 | Loss: 99.6883 | NLL: 43.6711 | KL: 213.6184
Epoch 0200 | Beta: 0.33 | Loss: 113.0450 | NLL: 47.7483 | KL: 195.6109
Epoch 0250 | Beta: 0.40 | Loss: 125.8601 | NLL: 54.4968 | KL: 179.1054
Epoch 0300 | Beta: 0.46 | Loss: 131.6116 | NLL: 53.1250 | KL: 171.8162
Epoch 0350 | Beta: 0.51 | Loss: 138.4112 | NLL: 55.3363 | KL: 163.0497
Epoch 0400 | Beta: 0.56 | Loss: 146.8346 | NLL: 61.1580 | KL: 153.7917
Epoch 0450 | Beta: 0.60 | Loss: 152.1430 | NLL: 64.0044 | KL: 146.8816
Epoch 0500 | Beta: 0.64 | Loss: 160.1281 | NLL: 73.5975 | KL: 135.4439
Epoch 0550 | Beta: 0.67 | Loss: 162.5976 | NLL: 79.0434 | KL: 123.9853
Epoch 0600 | Beta: 0.71 | Loss: 166.9954 | NLL: 91.0289 | KL: 107.6712
Epoch 0650 | Beta: 0.73 | Loss: 167.4525 | NLL: 88.5292 | KL: 107.5087
Epoch 0700 | Beta: 0.76 | Loss: 170.1903 | NLL: 89.5688 | KL: 106.0939
Epoch 0750 | Beta: 0.78 | Loss: 172.6075 | NLL: 90.1730 | KL: 105.2534
Epoch 0800 | Beta: 0.80 | Loss: 173.4447 | NLL: 89.2210 | KL: 104.7254
Epoch 0850 | Beta: 0.82 | Loss: 176.5056 | NLL: 91.0617 | KL: 103.7913
Epoch 0900 | Beta: 0.84 | Loss: 180.3924 | NLL: 95.7794 | KL: 100.6843
Epoch 0950 | Beta: 0.86 | Loss: 177.9588 | NLL: 89.1865 | KL: 103.7223
Epoch 1000 | Beta: 0.87 | Loss: 179.7792 | NLL: 90.9029 | KL: 102.1745
Correlation Heatmap saved to paper_plots\run_CPU2_20260105_132615/latent_heatmap.png

>>> Generating 1000 Monte Carlo Scenarios for VaR...

--- Kupiec POF Test ---
Totale Giorni: 1100
Violazioni Attese: 55.0
Violazioni Osservate: 29
Frequenza Osservata: 0.0264 (Target: 0.0500)
LR Statistic: 15.5194 | P-Value: 0.0001
RISULTATO: RIGETTO H0. Il modello NON è calibrato correttamente.

--- Christoffersen Conditional Coverage Test ---
Transizioni: n00=1044, n01=26, n10=27, n11=2
LR Independence: 1.5788 (p-value: 0.2089)
LR Conditional Coverage (CC): 17.0983 | P-Value CC: 0.0002
RISULTATO: RIGETTO H0. Il modello fallisce il test condizionale.
Grafico regimi salvato in 'paper_plots\run_CPU2_20260105_132615/var_regimes_plot.png'

>>> Analysis Complete.

------------------------------------------------

Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_142140
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_142140

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 48
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.01
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_142140
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.1
free_bits: 2.0
gradient_clip: 2.5
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...

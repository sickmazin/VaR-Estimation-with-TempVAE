Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_163813
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_163813

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 27
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.01
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_163813
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.2
free_bits: 2.0
gradient_clip: 4.5
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...
Epoch 0050 | Beta: 0.10 | Loss: 93.6809 | NLL: 71.4802 | KL: 233.2620
Epoch 0100 | Beta: 0.18 | Loss: 99.7221 | NLL: 64.7016 | KL: 191.4093
Epoch 0150 | Beta: 0.26 | Loss: 104.5921 | NLL: 61.6685 | KL: 163.6864
Epoch 0200 | Beta: 0.33 | Loss: 116.3006 | NLL: 66.3959 | KL: 149.5008
Epoch 0250 | Beta: 0.40 | Loss: 119.2142 | NLL: 65.3354 | KL: 135.2234
Epoch 0300 | Beta: 0.46 | Loss: 126.3407 | NLL: 65.9058 | KL: 132.2988
Epoch 0350 | Beta: 0.51 | Loss: 129.9681 | NLL: 63.8316 | KL: 129.8050
Epoch 0400 | Beta: 0.56 | Loss: 136.1493 | NLL: 63.7167 | KL: 130.0186
Epoch 0450 | Beta: 0.60 | Loss: 141.0576 | NLL: 64.4185 | KL: 127.7179
Epoch 0500 | Beta: 0.64 | Loss: 144.5584 | NLL: 63.6858 | KL: 126.5874
Epoch 0550 | Beta: 0.67 | Loss: 150.2261 | NLL: 65.6897 | KL: 125.4428
Epoch 0600 | Beta: 0.71 | Loss: 154.0142 | NLL: 66.7345 | KL: 123.7058
Epoch 0650 | Beta: 0.73 | Loss: 158.5913 | NLL: 67.4800 | KL: 124.1113
Epoch 0700 | Beta: 0.76 | Loss: 158.0279 | NLL: 64.7268 | KL: 122.7797
Epoch 0750 | Beta: 0.78 | Loss: 161.5729 | NLL: 66.7448 | KL: 121.0776
Epoch 0800 | Beta: 0.80 | Loss: 168.2787 | NLL: 71.7878 | KL: 119.9785
Epoch 0850 | Beta: 0.82 | Loss: 166.0761 | NLL: 67.4809 | KL: 119.7666
Epoch 0900 | Beta: 0.84 | Loss: 168.6511 | NLL: 67.8326 | KL: 119.9680
Epoch 0950 | Beta: 0.86 | Loss: 168.3676 | NLL: 65.7865 | KL: 119.8566
Epoch 1000 | Beta: 0.87 | Loss: 170.2591 | NLL: 66.4340 | KL: 119.3600
Correlation Heatmap saved to paper_plots\run_CPU2_20260105_163813/latent_heatmap.png

>>> Generating 1000 Monte Carlo Scenarios for VaR...

--- Kupiec POF Test ---
Totale Giorni: 1100
Violazioni Attese: 55.0
Violazioni Osservate: 20
Frequenza Osservata: 0.0182 (Target: 0.0500)
LR Statistic: 30.6953 | P-Value: 0.0000
RISULTATO: RIGETTO H0. Il modello NON è calibrato correttamente.

--- Christoffersen Conditional Coverage Test ---
Transizioni: n00=1060, n01=19, n10=19, n11=1
LR Independence: 0.7914 (p-value: 0.3737)
LR Conditional Coverage (CC): 31.4867 | P-Value CC: 0.0000
RISULTATO: RIGETTO H0. Il modello fallisce il test condizionale.
Grafico regimi salvato in 'paper_plots\run_CPU2_20260105_163813/var_regimes_plot.png'

>>> Analysis Complete.

------------------------------------------------

Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_173919
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_173919

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 28
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.01
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_173919
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.2
free_bits: 2.0
gradient_clip: 4.5
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...

Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_142458
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_142458

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 26
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.0
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_142458
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.1
free_bits: 2.0
gradient_clip: 4
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...
Epoch 0050 | Beta: 0.10 | Loss: 70.2028 | NLL: 39.2741 | KL: 324.9659
Epoch 0100 | Beta: 0.18 | Loss: 70.7254 | NLL: 24.6537 | KL: 251.8112
Epoch 0150 | Beta: 0.26 | Loss: 73.4327 | NLL: 18.3979 | KL: 209.8717
Epoch 0200 | Beta: 0.33 | Loss: 78.0655 | NLL: 16.3764 | KL: 184.8038
Epoch 0250 | Beta: 0.40 | Loss: 82.5662 | NLL: 15.2386 | KL: 168.9768
Epoch 0300 | Beta: 0.46 | Loss: 82.5765 | NLL: 9.5390 | KL: 159.8873
Epoch 0350 | Beta: 0.51 | Loss: 86.9246 | NLL: 8.8892 | KL: 153.1586
Epoch 0400 | Beta: 0.56 | Loss: 91.4774 | NLL: 8.7771 | KL: 148.4493
Epoch 0450 | Beta: 0.60 | Loss: 98.7859 | NLL: 11.6146 | KL: 145.2698
Epoch 0500 | Beta: 0.64 | Loss: 96.3798 | NLL: 9.4132 | KL: 136.1264
Epoch 0550 | Beta: 0.67 | Loss: 101.2104 | NLL: 11.7911 | KL: 132.6885
Epoch 0600 | Beta: 0.71 | Loss: 105.6551 | NLL: 14.0762 | KL: 129.7993
Epoch 0650 | Beta: 0.73 | Loss: 106.5906 | NLL: 11.7046 | KL: 129.2531
Epoch 0700 | Beta: 0.76 | Loss: 107.6725 | NLL: 12.7412 | KL: 124.9248
Epoch 0750 | Beta: 0.78 | Loss: 110.4730 | NLL: 11.5167 | KL: 126.3486
Epoch 0800 | Beta: 0.80 | Loss: 109.9776 | NLL: 11.3954 | KL: 122.5790
Epoch 0850 | Beta: 0.82 | Loss: 122.4285 | NLL: 21.8426 | KL: 122.1848
Epoch 0900 | Beta: 0.84 | Loss: 113.1237 | NLL: 12.8892 | KL: 119.2730
Epoch 0950 | Beta: 0.86 | Loss: 115.2767 | NLL: 13.4393 | KL: 118.9878
Epoch 1000 | Beta: 0.87 | Loss: 117.1753 | NLL: 15.0766 | KL: 117.3753
Correlation Heatmap saved to paper_plots\run_CPU2_20260105_142458/latent_heatmap.png

>>> Generating 1000 Monte Carlo Scenarios for VaR...

--- Kupiec POF Test ---
Totale Giorni: 1100
Violazioni Attese: 55.0
Violazioni Osservate: 45
Frequenza Osservata: 0.0409 (Target: 0.0500)
LR Statistic: 2.0350 | P-Value: 0.1537
RISULTATO: NON RIGETTO H0. Il modello è statisticamente Valido.

--- Christoffersen Conditional Coverage Test ---
Transizioni: n00=1014, n01=40, n10=41, n11=4
LR Independence: 2.2192 (p-value: 0.1363)
LR Conditional Coverage (CC): 4.2543 | P-Value CC: 0.1192
RISULTATO: NON RIGETTO H0. Il modello è Valido Condizionalmente.
Grafico regimi salvato in 'paper_plots\run_CPU2_20260105_142458/var_regimes_plot.png'

>>> Analysis Complete.

------------------------------------------------

Risolto il problema di nidificazione delle directory.
Attualmente la directory per i plot è: paper_plots\run_CPU2_20260105_152203
>>> Risultati di questa esecuzione salvati in: paper_plots\run_CPU2_20260105_152203

==================================================
CONFIGURAZIONE ESPERIMENTO
==================================================
seq_length: 29
latent_dim: 10
hidden_dim: 27
batch_size: 140
epochs: 1000
annealing_epochs: 100
learning_rate: 0.001
l2_reg: 0.0
data_path: dataset/log_returns.csv
plot_dir: paper_plots\run_CPU2_20260105_152203
device: cpu
seed: 42
train_split: 0.7
dropout_rate: 0.1
free_bits: 2.0
gradient_clip: 4
annealing_decay_rate: 0.96
annealing_steps: 20
==================================================

>>> 1. Loading Dataset...
--------------------------------------------------
Data Analysis Start Date: 2014-09-18
Data Analysis End Date:   2024-12-31
--------------------------------------------------
>>> 2. Starting Training Loop...
